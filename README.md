<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
   
</head>

<body>

<h1>MNIST Digit Classification using Machine Learning</h1>

<p>
This project implements a complete <b>end-to-end machine learning pipeline</b> on the
<b>MNIST handwritten digit dataset</b>.  
Multiple classification models are trained, evaluated, compared using
<b>ROCâ€“AUC curves</b>, and the best-performing model is further optimized using
<b>GridSearchCV</b>.
</p>

<hr>

<h2>ğŸ“Œ Project Workflow Overview</h2>

<div class="diagram">
Raw MNIST Data
      -->
Null Value Check
      -->
Train-Test Split
      -->
Model Training
      -->
Predictions & Probabilities
      -->
Evaluation Metrics
      -->
ROCâ€“AUC Curve Comparison
      -->
Best Model Selection (SVM)
      -->
Hyperparameter Tuning (GridSearchCV)
      -->
Final Evaluation
      -->
Model Serialization (Pickle)
</div>

<hr>

<h2>ğŸ“Š Dataset Description</h2>

<ul>
    <li>Dataset: <b>MNIST Handwritten Digits</b></li>
    <li>Classes: Digits from <b>0 to 9</b></li>
    <li>Each image is converted into numerical pixel values</li>
</ul>

<hr>

<h2>ğŸ§¹ Data Preprocessing</h2>

<div class="box">
<ul>
    <li>Checked for <b>null values</b> in the dataset</li>
    <li>Split the dataset into <b>training</b> and <b>testing</b> sets using <code>train_test_split</code></li>
    <li>Applied <b>label binarization</b> on test labels using <code>label_binarize</code></li>
</ul>
</div>

<p>
Label binarization converts multi-class labels into binary format, which is essential for:
</p>

<ul>
    <li>ROC curve plotting</li>
    <li>ROCâ€“AUC score calculation</li>
</ul>

<hr>

<h2>ğŸ¤– Machine Learning Models Used</h2>

<div class="box">
<ul>
    <li>K-Nearest Neighbors (KNN)</li>
    <li>Naive Bayes</li>
    <li>Logistic Regression</li>
    <li>Decision Tree</li>
    <li>Random Forest</li>
    <li>AdaBoost</li>
    <li>Gradient Boosting</li>
    <li>XGBoost</li>
    <li>Support Vector Machine (SVM)</li>
</ul>
</div>

<p>
Each model was implemented using <b>classes and objects</b> to ensure modular, reusable,
and clean code architecture.
</p>

<hr>

<h2>ğŸ“ˆ Model Training & Prediction</h2>

<ul>
    <li>Each model was trained on the training dataset</li>
    <li>Predictions were made on both training and testing data</li>
    <li>Performance was evaluated using:</li>
</ul>

<ul>
    <li><code>accuracy_score</code></li>
    <li><code>classification_report</code></li>
    <li><code>confusion_matrix</code></li>
</ul>

<hr>

<h2>ğŸ“‰ ROCâ€“AUC Curve Explanation</h2>

<div class="box">
<p>
For ROCâ€“AUC calculation:
</p>
<ul>
    <li>Probability predictions were obtained using <code>predict_proba()</code></li>
    <li>The true labels and predicted probabilities were flattened using <code>ravel()</code></li>
</ul>

<p>
Flattening converts 2D arrays into a 1D format, which helps in:
</p>

<ul>
    <li>Calculating False Positive Rate (FPR)</li>
    <li>Calculating True Positive Rate (TPR)</li>
    <li>Accurate AUC score computation</li>
</ul>
</div>



<p>
ROCâ€“AUC curves were plotted for <b>all models</b> to compare their performance visually.
</p>

<hr>

<h2>ğŸ† Best Model Selection</h2>

<p>
Based on ROCâ€“AUC score and overall performance:
</p>

<div class="box">
<h3>âœ… Support Vector Machine (SVM)</h3>
<ul>
    <li>Achieved the highest ROCâ€“AUC score</li>
    <li>Showed strong generalization on test data</li>
</ul>
</div>

<hr>

<h2>âš™ï¸ Hyperparameter Tuning</h2>

<p>
The SVM model was further optimized using <b>GridSearchCV</b>:
</p>

<ul>
    <li>Automatically searched for best hyperparameters</li>
    <li>Used cross-validation for robust performance</li>
</ul>

<p>
After tuning, the following were evaluated again:
</p>

<ul>
    <li>Training accuracy</li>
    <li>Training classification report</li>
    <li>Training confusion matrix</li>
    <li>Test accuracy</li>
    <li>Test classification report</li>
    <li>Test confusion matrix</li>
</ul>

<hr>

<h2>ğŸ’¾ Model Saving</h2>

<div class="box">
<p>
The final optimized SVM model was saved using <b>Pickle</b>, allowing the model to be
loaded later without retraining.
</p>
</div>

<hr>

<h2>ğŸ› ï¸ Libraries Used</h2>

<ul>
    <li>NumPy</li>
    <li>Pandas</li>
    <li>Matplotlib</li>
    <li>Scikit-learn</li>
     <li>K-Nearest Neighbors (KNN)</li>
    <li>Naive Bayes</li>
    <li>Logistic Regression</li>
    <li>Decision Tree</li>
    <li>Random Forest</li>
    <li>AdaBoost</li>
    <li>Gradient Boosting</li>
    <li>XGBoost</li>
    <li>Support Vector Machine (SVM)</li>
    <li>Accuracy Score</li>
    <li>Classification Report</li>
    <li>Confusion Matrix</li>
</ul>

<hr>

<h2>ğŸ“Œ Conclusion</h2>

<p>
This project demonstrates a <b>complete machine learning workflow</b> â€”
from preprocessing and model comparison to ROCâ€“AUC evaluation, hyperparameter tuning,
and model deployment readiness.
</p>

<p>
The modular design using classes and objects makes the project scalable, readable,
and suitable for real-world machine learning applications.
</p>

</body>
</html>
